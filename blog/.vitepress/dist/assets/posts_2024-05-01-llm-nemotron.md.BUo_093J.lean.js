import{_ as d}from"./chunks/giseldo.CX2LeuZ1.js";import{_ as p,c as h,j as e,a as s,t as l,a2 as o,G as t,w as n,B as m,o as k}from"./chunks/framework.Cc2F01qG.js";const v=JSON.parse('{"title":"LLM Nemotron 70B","description":"Postagem explicando o LLM da Nvidia Nemotron","frontmatter":{"date":"2024-05-01T00:00:00.000Z","title":"LLM Nemotron 70B","description":"Postagem explicando o LLM da Nvidia Nemotron","category":"Artigo","author":"Giseldo Neo","sidebar":false,"tags":["LLM","inteligência artificial"]},"headers":[],"relativePath":"posts/2024-05-01-llm-nemotron.md","filePath":"posts/2024-05-01-llm-nemotron.md","lastUpdated":1734134202000}'),u={name:"posts/2024-05-01-llm-nemotron.md"},c={id:"frontmatter-title",tabindex:"-1"};function g(r,a,E,y,F,f){const i=m("center");return k(),h("div",null,[e("h1",c,[s(l(r.$frontmatter.title)+" ",1),a[0]||(a[0]=e("a",{class:"header-anchor",href:"#frontmatter-title","aria-label":'Permalink to "{{ $frontmatter.title }}"'},"​",-1))]),e("p",null,l(r.$frontmatter.description),1),a[5]||(a[5]=o('<hr class="solid"><div class="profile"><img src="'+d+'" alt="Profile Picture"><div class="profile-details"><p>Giseldo Neo</p><p>01/01/2024</p></div></div><hr class="solid"><h2 id="introducao" tabindex="-1">Introdução <a class="header-anchor" href="#introducao" aria-label="Permalink to &quot;Introdução&quot;">​</a></h2><p>Em julho de 2024, a Meta lançou um modelo de linguagem (LLM) open-source, o <em>Llama-3.1-70B</em>. Um pouco depois, em setembro a empresa NVIDIA lançou um derivado deste, o <em>Llama 3.1-Nemotron-51B-Instruct</em>. E em outubro lançou finalmente um modelo de 70b, o <em>Llama 3.1 nemotron-70b-instruct</em>.</p><p>O <em>nemotron-70b</em> performou melhor, em alguns testes comparativos, do que o GPT-4o. Nos testes, ele liderou em desempenho geral e também se destacou nas categorias chat (<em>chat score</em>) e raciocínio (<em>reasoning score</em>). Veja na <strong>Tabela 1</strong> os dados comparativos com mais detalhes.</p>',6)),t(i,null,{default:n(()=>a[1]||(a[1]=[s("**Tabela 1** - Comparativo entre Neumotron e GPT4o.")])),_:1}),a[6]||(a[6]=o('<table tabindex="0"><thead><tr><th>Model</th><th style="text-align:right;">Overall Score</th><th style="text-align:right;">Chat Score</th><th style="text-align:right;">Reasoning Score</th></tr></thead><tbody><tr><td>Llama 3.1 Nemotron-70B</td><td style="text-align:right;">94.1</td><td style="text-align:right;">97.5</td><td style="text-align:right;">98.1</td></tr><tr><td>Skywork-Reward-Gemma-2-27B</td><td style="text-align:right;">93.8</td><td style="text-align:right;">95.8</td><td style="text-align:right;">96.1</td></tr><tr><td>TextEval-Llama3.1-70B</td><td style="text-align:right;">93.5</td><td style="text-align:right;">94.1</td><td style="text-align:right;">96.4</td></tr><tr><td>GPT-4o</td><td style="text-align:right;">86.7</td><td style="text-align:right;">96.1</td><td style="text-align:right;">86.6</td></tr></tbody></table>',1)),t(i,null,{default:n(()=>a[2]||(a[2]=[s("Fonte: [Bind AI](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct?snippet_tab=Node)")])),_:1}),a[7]||(a[7]=o(`<h2 id="testes-de-uso" tabindex="-1">Testes de uso <a class="header-anchor" href="#testes-de-uso" aria-label="Permalink to &quot;Testes de uso&quot;">​</a></h2><p>O Nemontron pode ser testado em <a href="https://build.nvidia.com" target="_blank" rel="noreferrer">build.nvidia.com</a>, especificamente <a href="https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct?snippet_tab=Node" target="_blank" rel="noreferrer">neste link</a>. A inscrição concede acesso a 100.000 chamadas de API gratuitas. Além disso, ele está disponível para baixar no <a href="https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct" target="_blank" rel="noreferrer">Hugging Face</a>. Porém, haja poder de processamento para uma consulta local a este modelo em uma PC de mesa.</p><p>Realizei algumas consultas no site da NVIDIA (<strong>Figura 1</strong>) e a velocidade da resposta deixou muito a desejar em relação a quantidade de tokens por minuto. Um versão do <a href="https://console.groq.com/playground" target="_blank" rel="noreferrer">Groq</a> disponível no Groq Cloud (<em>Figura 2</em>), que usa uma versão também customizada do Llama, o <em>llama3-groq-70b-8192-tool-use-preview</em>, é muito mais veloz do que a disponibilizada no site da NVIDIA. Provavelmente isto está relacionado mais ao hardware da Groq, que parece ser mais eficiente, independente do modelo. Cabe ressaltar que a versão da NVIDIA deu uma resposta bem mais completa que a versão do Groq, para o mesmo prompt.</p><p><strong>Figura 1</strong> - Demonstração do Nemotron 70B no site da NVidia</p><p>Fonte: O Autor (2024)</p><p><strong>Figura 2</strong> - Demonstração no Groq Cloud do modelo 70B tool use preview</p><p>Fonte: O Autor (2024)</p><h2 id="arquitetura" tabindex="-1">Arquitetura <a class="header-anchor" href="#arquitetura" aria-label="Permalink to &quot;Arquitetura&quot;">​</a></h2><p>Um LLM geralmente utiliza a arquitetura <em>transformer</em>, e no Nemotron não foi diferente. Esta arquitetura já é conhecida e permite que o modelo capture dependências de longo alcance no texto, tornando-o apto a compreender o contexto e a gerar respostas.</p><p>Outro recurso utilizado é o <em>Attention Multi-Head</em> que permite que o modelo se concentre em diferentes partes da entrada simultaneamente, aprimorando sua capacidade de compreender consultas complexas e produzir resultados diferenciados.</p><p>Por fim, foi implementado no modelo, a <em>normalização de camadas</em>. Ela ajuda a estabilizar o treinamento e a melhorar as taxas de convergência, resultando em um aprendizado mais rápido e eficiente. o modelo foi treinado em uma ampla gama de dados licenciados com a licença (CC-BY-4.0), que inclui livros, artigos e conteúdo da web. Ressaltando que o BY da licença exige que o crédito seja dado ao autor.</p><p>De acordo com a NVIDIA, o processo de treinamento do <em>Llama 3.1 Nemotron-70B</em> incluiu:</p><ul><li>aprendizagem supervisionada</li><li>aprendizagem por reforço de feedback humano.</li><li>Modelagem de recompensa: O modelo prevê a qualidade da resposta com base nas interações do usuário. Este mecanismo permite ajustar seus resultados de forma dinâmica, melhorando ao longo do tempo com base no feedback do mundo real.</li></ul><h2 id="codigo" tabindex="-1">Código <a class="header-anchor" href="#codigo" aria-label="Permalink to &quot;Código&quot;">​</a></h2><p>Em relação a código, a consulta ao modelo em python utiliza a mesma API do LLM da OpenAI (conforme pode ser visto no código abaixo), alterando somente a <em>base url</em>.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;https://integrate.api.nvidia.com/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">completion </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;nvidia/llama-3.1-nemotron-70b-instruct&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;What is machine learning?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  top_p</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  max_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1024</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">  stream</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> completion:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].delta.content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> not</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(chunk.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].delta.content, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">end</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>Curtiu? Deixei um comentário. Até o próximo post.</p><h3 id="referencias" tabindex="-1">Referências <a class="header-anchor" href="#referencias" aria-label="Permalink to &quot;Referências&quot;">​</a></h3><ul><li><a href="https://blog.getbind.co/2024/10/17/llama-3-1-nemotron-70b-is-it-better-for-coding-compared-to-gpt-4o-and-claude-3-5-sonnet/" target="_blank" rel="noreferrer">Bind AI Blog post</a></li><li><a href="https://creativecommons.org/share-your-work/cclicenses/" target="_blank" rel="noreferrer">CC-BY</a></li><li><a href="https://console.groq.com/" target="_blank" rel="noreferrer">GROQ</a></li><li><a href="https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b" target="_blank" rel="noreferrer">NVIDIA Blog post</a></li><li><a href="https://www.reddit.com/r/LocalLLaMA/comments/1fnp2kt/new_llama31nemotron51b_instruct_model_from_nvidia/" target="_blank" rel="noreferrer">Reddit post</a></li></ul>`,19)),t(i,null,{default:n(()=>a[3]||(a[3]=[s(". . .")])),_:1}),e("p",null,[e("em",null,[t(i,null,{default:n(()=>a[4]||(a[4]=[s("Até o próximo artigo")])),_:1})])])])}const A=p(u,[["render",g]]);export{v as __pageData,A as default};
