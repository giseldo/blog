import{_ as n}from"./chunks/giseldo.CX2LeuZ1.js";import{_ as p,c as u,j as a,a as o,t as d,a2 as l,G as t,w as i,B as m,o as c}from"./chunks/framework.Cc2F01qG.js";const g="/blog/assets/neuronio.CFQc3HHN.png",_=JSON.parse('{"title":"Redes Neurais (2#)","description":"Função de perda, retropropagação e gradiente","frontmatter":{"date":"2024-12-12T00:00:00.000Z","title":"Redes Neurais (2#)","description":"Função de perda, retropropagação e gradiente","category":"Documentação","author":"Giseldo Neo","sidebar":false,"tags":["redes neurais","aprendizagem de máquina"]},"headers":[],"relativePath":"posts/2024-12-12-redes-neurais-2.md","filePath":"posts/2024-12-12-redes-neurais-2.md","lastUpdated":1734134202000}'),f={name:"posts/2024-12-12-redes-neurais-2.md"},b={id:"frontmatter-title",tabindex:"-1"};function h(r,e,v,q,k,x){const s=m("center");return c(),u("div",null,[a("h1",b,[o(d(r.$frontmatter.title)+" ",1),e[0]||(e[0]=a("a",{class:"header-anchor",href:"#frontmatter-title","aria-label":'Permalink to "{{ $frontmatter.title }}"'},"​",-1))]),a("p",null,d(r.$frontmatter.description),1),e[3]||(e[3]=l('<hr class="solid"><div class="profile"><img src="'+n+'" alt="Profile Picture"><div class="profile-details"><p>Giseldo Neo</p><p>12/12/2024</p></div></div><hr class="solid"><h2 id="introducao" tabindex="-1">Introdução <a class="header-anchor" href="#introducao" aria-label="Permalink to &quot;Introdução&quot;">​</a></h2><p>Este texto é uma continuação do post anterior <a href="/blog/posts/2024-04-01-redes-neurais-1.html">redes neurais #1</a>.</p><p>As redes neurais aprendem ajustando pesos e vieses com base nos dados fornecidos e nos resultados obtidos. Esses pesos e vieses determinam como os dados de entrada influenciam o resultado final do modelo. Em problemas de classificação, por exemplo, a rede pode gerar como saída um vetor de probabilidades para cada classe. A calibração desses pesos e vieses ocorre por meio de um processo chamado de <strong>gradiente do erro</strong>, que se baseia no desempenho da rede medido por uma função de perda.</p><p>Na Figura 1 é apresentado um neurônio artificial K e seus elementos: a função de ativação (<em>activation function</em>), os pesos <em>Wk1 até Wkn</em>, e o bias (<em>Bias</em>) (b), além das entradas e as saídas.</p><blockquote><p>Figura 1 - Um neurônio artificial <img src="'+g+'" alt="Modelo não linear de um neurônio"> Fonte: Neural Networks and Learning Machines. Simon Hayking (2024)</p></blockquote><h2 id="funcao-de-perda" tabindex="-1">Função de perda <a class="header-anchor" href="#funcao-de-perda" aria-label="Permalink to &quot;Função de perda&quot;">​</a></h2><p>A <strong>função de perda</strong> é uma métrica que avalia o quão bem a rede está performando. Para problemas de regressão, uma função comum é o <strong>erro quadrático médio</strong>, que mede a diferença entre os valores previstos e os reais. Já para problemas de classificação, utiliza-se frequentemente a <strong>perda por entropia cruzada</strong>, que considera as probabilidades das classes previstas. Com base na perda calculada, inicia-se o processo de <strong>retropropagação</strong>.</p><h2 id="retropropagacao" tabindex="-1">Retropropagação <a class="header-anchor" href="#retropropagacao" aria-label="Permalink to &quot;Retropropagação&quot;">​</a></h2><p>A <strong>retropropagação</strong> consiste em atualizar os pesos e vieses da rede a partir dos gradientes da função de perda com relação a essas variáveis. Esse processo utiliza a <strong>regra da cadeia</strong>, pois o cálculo dos gradientes envolve derivadas parciais acumuladas ao longo das camadas da rede. Inicialmente, os pesos e vieses são configurados aleatoriamente e, durante a retropropagação, ajustam-se para minimizar a perda. O objetivo é determinar se os valores atuais devem ser aumentados ou reduzidos e em que proporção.</p><h2 id="descida-do-gradiente" tabindex="-1">Descida do gradiente <a class="header-anchor" href="#descida-do-gradiente" aria-label="Permalink to &quot;Descida do gradiente&quot;">​</a></h2><p>O mecanismo que conduz esses ajustes é conhecido como <strong>descida do gradiente</strong>, que busca o ponto mínimo da função de perda. Intuitivamente, o gradiente indica a inclinação da função em relação aos pesos e vieses, permitindo que a rede se mova na direção que reduz o erro. Se o gradiente for positivo, ajusta-se para um valor menor; se negativo, ajusta-se para um valor maior. O processo de ajuste é influenciado pela <strong>taxa de aprendizado</strong>, que controla a magnitude dos passos dados na direção do mínimo.</p><p>Durante o aprendizado, cada neurônio recebe um dado de entrada, o multiplica pelos pesos, soma o viés e aplica uma função de ativação, produzindo uma saída. Esse processo se repete para todas as camadas até gerar a saída final, que será avaliada pela função de perda. Com os ajustes feitos na retropropagação, os dados são novamente passados pela rede, repetindo o ciclo até atingir a perda desejada ou o número máximo de iterações.</p><p>Esse processo iterativo, essencial para redes neurais, permite que elas aprendam padrões complexos nos dados e ajustem seus parâmetros para alcançar um desempenho otimizado. A combinação de conceitos como função de perda, retropropagação, descida do gradiente e taxa de aprendizado forma a base do aprendizado em redes neurais.</p><h2 id="referencias" tabindex="-1">Referências <a class="header-anchor" href="#referencias" aria-label="Permalink to &quot;Referências&quot;">​</a></h2><p><a href="https://www.aibutsimple.com/p/neural-networks-learning-parameters-weights-biases-backprop" target="_blank" rel="noreferrer">ai but simple Issue 2</a></p>',18)),t(s,null,{default:i(()=>e[1]||(e[1]=[o(". . .")])),_:1}),a("p",null,[a("em",null,[t(s,null,{default:i(()=>e[2]||(e[2]=[o("Até a próxima postagem")])),_:1})])])])}const j=p(f,[["render",h]]);export{_ as __pageData,j as default};
